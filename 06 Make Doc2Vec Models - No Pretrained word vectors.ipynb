{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This file will make Doc2Vec models from the input. It will not use pretrained vectors'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description = \"\"\"This file will make Doc2Vec models from the input. It will not use pretrained vectors\"\"\"\n",
    "description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "import sklearn\n",
    "from sklearn import cross_validation\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy import stats\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding(\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(raw_text):\n",
    "    review_text = BeautifulSoup(raw_text).get_text()\n",
    "    words = review_text.lower().split()\n",
    "    return(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = \"summary-english-truth.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(datafile, header=0, delimiter=\"\\t\", quoting=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_text = train[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"only_tweets.txt\", 'w') as out_file:\n",
    "    for each_line in all_text:\n",
    "        out_file.write(clean_text(each_line)+\"\\n\")\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc2veclines = gensim.models.doc2vec.TaggedLineDocument('only_tweets.txt')\n",
    "lines = []\n",
    "for each in doc2veclines:\n",
    "    lines.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simple_models = [\n",
    "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    \n",
    "    Doc2Vec(dm=1, dm_concat=1, size=25, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    Doc2Vec(dm=0, size=25, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=50, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    Doc2Vec(dm=0, size=50, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=150, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    Doc2Vec(dm=0, size=150, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=200, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    Doc2Vec(dm=0, size=200, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simple_models[0].build_vocab(lines)  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "simple_models[1].build_vocab(lines)\n",
    "simple_models[2].build_vocab(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(simple_models[0])\n",
    "for model in simple_models[1:]:\n",
    "    model.reset_from(simple_models[0])\n",
    "    print(model)\n",
    "    \n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample\n",
    "\n",
    "# for timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "#def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "#    logit = sm.Logit(train_targets, train_regressors)\n",
    "#    predictor = logit.fit(disp=0)\n",
    "#    #print(predictor.summary())\n",
    "#    return predictor\n",
    "\n",
    "#def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "#    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "#\n",
    "#    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "#    train_regressors = sm.add_constant(train_regressors)\n",
    "#    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "#\n",
    "#    test_data = test_set\n",
    "#    if infer:\n",
    "#        if infer_subsample < 1.0:\n",
    "#            test_data = sample(test_data, int(infer_subsample * len(test_data, b , b )))\n",
    "#        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "#    else:\n",
    "#        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "#    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "#    # predict & evaluate\n",
    "#    test_predictions = predictor.predict(test_regressors)\n",
    "#    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "#    errors = len(test_predictions) - corrects\n",
    "#    error_rate = float(errors) / len(test_predictions)\n",
    "#    return (error_rate, errors, len(test_predictions), predictor)\n",
    "\n",
    "\n",
    "def error_rate_for_model(test_model, target):\n",
    "    num_instances = len(target)\n",
    "    dims = test_model.docvecs[0].shape[0]\n",
    "    # preallocate\n",
    "    train_x = np.zeros((num_instances, dims))\n",
    "    \n",
    "    for i in range(0, num_instances):\n",
    "        train_x[i,:] = test_model.docvecs[i]\n",
    "    \n",
    "    scoring_function='accuracy'\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    scores = cross_validation.cross_val_score(clf, train_x, target, cv=10, scoring=scoring_function)\n",
    "    err = (1.0 - scores.mean())\n",
    "    \n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "best_error = defaultdict(lambda :1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
    "alpha_delta = (alpha - min_alpha) / passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"START %s\" % datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = train[\"age\"]\n",
    "for epoch in range(passes):\n",
    "    shuffle(lines)  # shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        with elapsed_timer() as elapsed:\n",
    "            train_model.train(lines)\n",
    "            duration = '%.1f' % elapsed()\n",
    "            \n",
    "        # evaluate\n",
    "        eval_duration = ''\n",
    "        with elapsed_timer() as eval_elapsed:\n",
    "            err = error_rate_for_model(train_model, target)\n",
    "        eval_duration = '%.1f' % eval_elapsed()\n",
    "        \n",
    "        best_indicator = ' '\n",
    "        if err <= best_error[name]:\n",
    "            best_error[name] = err\n",
    "            best_indicator = '*' \n",
    "        print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, err, epoch + 1, name, duration, eval_duration))\n",
    "\n",
    "#        if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "#            eval_duration = ''\n",
    "#            with elapsed_timer() as eval_elapsed:\n",
    "#                infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
    "#            eval_duration = '%.1f' % eval_elapsed()\n",
    "#            best_indicator = ' '\n",
    "#            if infer_err < best_error[name + '_inferred']:\n",
    "#                best_error[name + '_inferred'] = infer_err\n",
    "#                best_indicator = '*'\n",
    "#            print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, infer_err, epoch + 1, name + '_inferred', duration, eval_duration))\n",
    "\n",
    "    print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
